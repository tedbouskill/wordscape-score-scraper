{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR Prototypes\n",
    "\n",
    "Rerun the first code cell to reload the files if new files are added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tedbouskill/Repos/MyGitHub/wordscape-score-scraper/.venv/bin/python\n",
      "3.11.11 | packaged by conda-forge | (main, Dec  5 2024, 14:24:23) [Clang 18.1.8 ]\n",
      "Common Files Setup()\n",
      "Looking for files in /Users/tedbouskill/Repos/MyGitHub/wordscape-score-scraper/prototypes/../png_samples\n",
      "Number of files found: 8\n",
      "Common Files Setup()\n",
      "Looking for files in /Users/tedbouskill/Repos/MyGitHub/wordscape-score-scraper/prototypes/../png_samples\n",
      "Number of files: 8\n",
      "1.26.4\n",
      "4.10.0\n"
     ]
    }
   ],
   "source": [
    "# Install the ipynb package\n",
    "#%pip install ipynb\n",
    "\n",
    "# Base code for all notebooks in the same folder\n",
    "from ipynb.fs.full.common import setup, reload_packages # type: ignore\n",
    "\n",
    "files = []\n",
    "files = setup()\n",
    "\n",
    "print(f\"Number of files: {len(files)}\")\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "print(np.__version__)\n",
    "print(cv2.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload Packages()\n",
    "\n",
    "Rerun the cell below this one to reload common packages in repo_packages or workspace_packages if they change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading packages\n"
     ]
    }
   ],
   "source": [
    "reload_packages()\n",
    "\n",
    "from cls_img_tools import ImageTools\n",
    "\n",
    "def no_op():\n",
    "    pass\n",
    "\n",
    "def scale_image(img, scale_factor):\n",
    "    img_height, img_width = img.shape[:2]\n",
    "    new_size = (int(img_width * scale_factor), int(img_height * scale_factor))\n",
    "    return cv2.resize(img, new_size, interpolation=cv2.INTER_AREA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from cls_env_tools import EnvTools\n",
    "\n",
    "# Function to convert transparent background to white\n",
    "def convert_transparent_to_white(image):\n",
    "    if len(image.shape) == 3 and image.shape[2] == 4:  # Check if the image has an alpha channel\n",
    "        alpha_channel = image[:, :, 3]\n",
    "        rgb_channels = image[:, :, :3]\n",
    "\n",
    "        # Create a white background image\n",
    "        white_background = np.ones_like(rgb_channels, dtype=np.uint8) * 255\n",
    "\n",
    "        # Blend the image with the white background using the alpha channel as a mask\n",
    "        alpha_factor = alpha_channel[:, :, np.newaxis] / 255.0\n",
    "        blended_image = rgb_channels * alpha_factor + white_background * (1 - alpha_factor)\n",
    "        return blended_image.astype(np.uint8)\n",
    "    elif len(image.shape) == 3 and image.shape[2] == 3:  # Image is already RGB\n",
    "        return image\n",
    "    else:  # Image is grayscale\n",
    "        return cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "# Function to visualize the result using matplotlib\n",
    "def visualize_result(image, title=\"Result\"):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Find the root directory and construct the path to the sample image\n",
    "star_img_path = os.path.join(EnvTools.find_repo_root(), \"star.png\")\n",
    "star_img = cv2.imread(star_img_path, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "if star_img is None:\n",
    "    print(f\"Error: Could not load sample image from {star_img_path}\")\n",
    "else:\n",
    "    # Convert transparent background to white\n",
    "    star_img = convert_transparent_to_white(star_img)\n",
    "    star_img_gray = cv2.cvtColor(star_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Initialize SIFT detector\n",
    "    sift = cv2.SIFT_create()\n",
    "\n",
    "    # Detect and compute keypoints and descriptors for the sample image\n",
    "    kp1, des1 = sift.detectAndCompute(star_img_gray, None)\n",
    "\n",
    "    if des1 is None:\n",
    "        print(\"Error: No descriptors found in the sample image.\")\n",
    "        visualize_result(star_img_gray, title=\"Sample Image Keypoints\")\n",
    "    else:\n",
    "        # Visualize keypoints in the sample image\n",
    "        img_with_keypoints = cv2.drawKeypoints(star_img_gray, kp1, None, color=(0, 255, 0))\n",
    "        visualize_result(img_with_keypoints, title=\"Sample Image Keypoints\")\n",
    "\n",
    "        # Initialize the BFMatcher\n",
    "        bf = cv2.BFMatcher()\n",
    "\n",
    "        # Assuming 'files' is a list of image file paths\n",
    "        for img_file in files:\n",
    "            print(f\"Processing {img_file}\")\n",
    "\n",
    "            image = cv2.imread(img_file, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "            if image is None:\n",
    "                print(f\"Error: Could not load image from {img_file}\")\n",
    "                continue\n",
    "\n",
    "            # Convert transparent background to white\n",
    "            image = convert_transparent_to_white(image)\n",
    "            image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # Detect and compute keypoints and descriptors for the current image\n",
    "            kp2, des2 = sift.detectAndCompute(image_gray, None)\n",
    "\n",
    "            if des2 is None:\n",
    "                print(f\"No descriptors found in image {img_file}.\")\n",
    "                img_with_keypoints = cv2.drawKeypoints(image_gray, kp2, None, color=(0, 255, 0))\n",
    "                visualize_result(img_with_keypoints, title=f\"Keypoints in {img_file}\")\n",
    "                continue\n",
    "\n",
    "            # Match descriptors\n",
    "            matches = bf.match(des1, des2)\n",
    "\n",
    "            if not matches:\n",
    "                print(f\"No matches found between sample image and {img_file}.\")\n",
    "                continue\n",
    "\n",
    "            # Sort matches by distance (lower distance is better)\n",
    "            matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "            # Extract location of good matches\n",
    "            src_pts = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 2)\n",
    "            dst_pts = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 2)\n",
    "\n",
    "            # Find homography\n",
    "            M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "            if M is not None:\n",
    "                h, w = star_img_gray.shape\n",
    "                pts = np.float32([[0, 0], [0, h - 1], [w - 1, h - 1], [w - 1, 0]]).reshape(-1, 1, 2)\n",
    "                dst = cv2.perspectiveTransform(pts, M)\n",
    "\n",
    "                # Draw bounding box around the detected region\n",
    "                image = cv2.polylines(image, [np.int32(dst)], True, (0, 255, 0), 3, cv2.LINE_AA)\n",
    "\n",
    "            # Draw matches\n",
    "            img_matches = cv2.drawMatches(star_img, kp1, image, kp2, matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "            # Visualize the result\n",
    "            visualize_result(img_matches, title=f\"Matching Result for {img_file}\")\n",
    "\n",
    "            print(f\"Matching Image: {img_file}\")\n",
    "            print(f\"Good Matches: {len(matches)}\")\n",
    "\n",
    "            break\n",
    "\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dis\n",
    "from math import e\n",
    "import cv2 # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "\n",
    "from PIL import Image # type: ignore\n",
    "\n",
    "# Boxes should be ~1218 x ~208 pixels\n",
    "# Or ~1220 x ~256 pixels\n",
    "\n",
    "def preprocess_image(img_path):\n",
    "    with Image.open(img_path) as img:\n",
    "        img = img.convert(\"RGB\")\n",
    "        data = list(img.getdata())\n",
    "        img_without_metadata = Image.new(img.mode, img.size)\n",
    "        img_without_metadata.putdata(data)\n",
    "        # Convert to NumPy array and then to OpenCV format (BGR)\n",
    "        img_np = np.array(img_without_metadata)\n",
    "        img_cv = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
    "        return img_cv\n",
    "\n",
    "def grayscale_normalize_image(img):\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    return cv2.normalize(gray_img, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "def transform_image(img):\n",
    "    gn_img = grayscale_normalize_image(img)\n",
    "    equalized = cv2.equalizeHist(gn_img)\n",
    "    return equalized\n",
    "\n",
    "def enhance_lines(img, original_img):\n",
    "    # Apply Canny edge detection\n",
    "    edges = cv2.Canny(img, 50, 150)\n",
    "\n",
    "    # Dilate the edges to enhance thin lines\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    dilated_edges = cv2.dilate(edges, kernel, iterations=1)\n",
    "\n",
    "    # Convert dilated edges to a 3-channel image\n",
    "    dilated_edges_colored = cv2.cvtColor(dilated_edges, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    # Combine the enhanced edges with the original image\n",
    "    enhanced_img = cv2.addWeighted(original_img, 0.8, dilated_edges_colored, 0.2, 0)\n",
    "\n",
    "    return enhanced_img\n",
    "\n",
    "def enhance_contrast(img):\n",
    "    # Enhance contrast with CLAHE\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    contrast_enhanced = clahe.apply(img)\n",
    "    def adjust_gamma(img, gamma=1.5):\n",
    "        inv_gamma = 1.0 / gamma\n",
    "        table = np.array([((i / 255.0) ** inv_gamma) * 255\n",
    "                        for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "        return cv2.LUT(img, table)\n",
    "    return adjust_gamma(contrast_enhanced, gamma=1.5)\n",
    "\n",
    "def display_image(img, title=\"Image\"):\n",
    "    # Convert the image from BGR (OpenCV format) to RGB (matplotlib format)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Rotate the image 90 degrees clockwise for display\n",
    "    img_rgb = cv2.rotate(img_rgb, cv2.ROTATE_90_CLOCKWISE)\n",
    "\n",
    "    # Display the image using matplotlib\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')  # Hide the axis\n",
    "    plt.show()\n",
    "\n",
    "for img_file in files:\n",
    "    print(f\"Processing {img_file}\")\n",
    "\n",
    "    #img = cv2.imread(img_file) #, cv2.IMREAD_UNCHANGED)  # Read the image\n",
    "    img = preprocess_image(img_file)\n",
    "\n",
    "    img_height, img_width, channels = img.shape\n",
    "    print(f\"\\tOriginal image width: {img_width}, height: {img_height}, channels: {channels}\")\n",
    "\n",
    "    if img_width == 296:\n",
    "        print(f\"\\tResizing image\")\n",
    "        img = scale_image(img, 4)\n",
    "        img_height, img_width = img.shape[:2]\n",
    "\n",
    "    t_img = transform_image(img)\n",
    "    #display_image(t_img, title=\"Transformed Image\")\n",
    "\n",
    "    # Enhance thin white lines\n",
    "    enhanced_lines = enhance_lines(t_img, img)\n",
    "    #display_image(enhanced_lines, title=\"Enhanced Image\")\n",
    "    enhanced = cv2.cvtColor(enhanced_lines, cv2.COLOR_BGR2GRAY)\n",
    "    #display_image(enhanced, title=\"Enhanced Grayscale Image\")\n",
    "\n",
    "    #enhanced = enhance_contrast(t_img)\n",
    "    #display_image(enhanced, title=\"Enhanced Image\")\n",
    "\n",
    "    # Blur to reduce noise (experiment with different blur types and kernel sizes)\n",
    "    blurred = cv2.GaussianBlur(enhanced, (3, 3), 0)\n",
    "    #display_image(blurred, title=\"Blurred Image\")\n",
    "\n",
    "    #enhanced = blurred\n",
    "\n",
    "    if (1 == 1):\n",
    "        #    # Threshold to create a binary image (white lines on black background)\n",
    "        _, binary = cv2.threshold(enhanced, 190, 255, cv2.THRESH_BINARY)\n",
    "    elif (1 == 0):\n",
    "        # Calculate a relative threshold based on image statistics\n",
    "        mean_intensity = np.mean(enhanced)\n",
    "        threshold_value = int(mean_intensity * 0.9)  # Adjust the factor (0.8) as needed\n",
    "        # Use the relative threshold in adaptiveThreshold\n",
    "        binary = cv2.adaptiveThreshold(\n",
    "            enhanced,\n",
    "            threshold_value,  # Relative threshold\n",
    "            cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "            cv2.THRESH_BINARY,\n",
    "            11,\n",
    "            2\n",
    "        )\n",
    "    else:\n",
    "        # Use adaptive thresholding for binarization\n",
    "        binary = cv2.adaptiveThreshold(\n",
    "            enhanced,\n",
    "            200,  # Maximum value for thresholding\n",
    "            cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "            cv2.THRESH_BINARY,\n",
    "            11,\n",
    "            2\n",
    "        )\n",
    "    display_image(binary, title=\"Binary Image\")\n",
    "\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    for contour in contours:\n",
    "        epsilon = 0.02 * cv2.arcLength(contour, True)\n",
    "        approx = cv2.approxPolyDP(contour, epsilon, True)\n",
    "\n",
    "        area = cv2.contourArea(contour)\n",
    "\n",
    "        # Check for rectangular-like shapes with rounded corners\n",
    "        if len(approx) > 4 and area > 5000:\n",
    "            # Get bounding rectangle and aspect ratio\n",
    "            x, y, w, h = cv2.boundingRect(approx)\n",
    "            aspect_ratio = w / float(h)\n",
    "            if 4.0 < aspect_ratio < 6.0: #and (200 <= h <= 270) and (1200 <= w <= 1240):  # Aspect ratio and size for rectangle\n",
    "                print(f\"area({area})\")\n",
    "                # Draw contours with bright orange color and thicker line\n",
    "                cv2.drawContours(img, [contour], -1, (255, 140, 0), 5)\n",
    "\n",
    "    display_image(img, title=\"Detected Boxes\")\n",
    "\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
